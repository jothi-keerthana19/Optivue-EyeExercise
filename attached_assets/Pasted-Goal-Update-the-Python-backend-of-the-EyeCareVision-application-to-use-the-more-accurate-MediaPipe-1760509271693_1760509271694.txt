Goal:

Update the Python backend of the EyeCareVision application to use the more accurate MediaPipe Face Detection model. The new implementation must provide real-time visual feedback (a bounding box) on the video stream served to the frontend.

Context:

The project has a Python Flask backend located in the EyeCareVision/server/ directory. The file enhanced_eye_tracker.py contains the computer vision logic, and enhanced_eye_tracking_server.py runs the web server. The current implementation is not accurate enough. We need to replace the logic with a new version that uses mp.solutions.face_detection for better performance and clear visual debugging.

Instructions:

Step 1: Update the Dependencies

Modify the EyeCareVision/pyproject.toml file to ensure the following dependencies are listed under [tool.poetry.dependencies]:

Ini, TOML

[tool.poetry.dependencies]
python = "^3.11"
Flask = "^3.0.3"
Flask-Cors = "^4.0.1"
opencv-python = "^4.10.0.84"
mediapipe = "^0.10.14"
numpy = "^1.26.4"
# (Keep any other dependencies you have)
Step 2: Update the Face Tracker Logic

Replace the entire content of the file EyeCareVision/server/enhanced_eye_tracker.py with the following code. This new code uses the FaceDetection model and includes logic for drawing visual feedback.

Python

import cv2
import numpy as np
import mediapipe as mp
from typing import Dict, Tuple, Any

class EnhancedEyeTracker:
    """
    A robust eye tracker using MediaPipe's dedicated FaceDetection model for high accuracy.
    This class processes a video frame to detect a face and annotates the frame with
    visual feedback (bounding box, keypoints, and confidence score).
    """

    def __init__(self, model_selection: int = 1, min_detection_confidence: float = 0.7) -> None:
        """
        Initializes the tracker with the MediaPipe FaceDetection model.

        Args:
            model_selection (int): 0 for short-range model (2 meters), 1 for full-range (5 meters).
                                   1 is generally more versatile and accurate.
            min_detection_confidence (float): Minimum confidence value (from 0.0 to 1.0) for a
                                              detection to be considered successful. A higher value
                                              like 0.7 increases accuracy by filtering out weak detections.
        """
        self.mp_face_detection = mp.solutions.face_detection
        
        # Initialize the FaceDetection model with the specified confidence
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=model_selection,
            min_detection_confidence=min_detection_confidence
        )
        print(f"MediaPipe FaceDetection initialized with confidence={min_detection_confidence}")

    def process_and_draw_frame(self, frame: np.ndarray) -> Tuple[Dict[str, Any], np.ndarray]:
        """
        Detects faces in a frame and draws detailed visual feedback.

        Args:
            frame (np.ndarray): The input image frame from the camera (in BGR format).

        Returns:
            A tuple containing:
            - A dictionary with detection results ('face_detected', 'success').
            - The annotated frame with a bounding box, keypoints, and score.
        """
        annotated_frame = frame.copy()
        frame_height, frame_width, _ = annotated_frame.shape

        # Convert the BGR image to RGB as MediaPipe expects this format
        rgb_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        
        # Process the frame to find faces
        results = self.face_detection.process(rgb_frame)
        
        face_detected = False
        
        if results.detections:
            face_detected = True
            # Loop through each detected face
            for detection in results.detections:
                # --- 1. Draw the Bounding Box ---
                bbox_data = detection.location_data.relative_bounding_box
                face_rect = np.multiply(
                    [bbox_data.xmin, bbox_data.ymin, bbox_data.width, bbox_data.height],
                    [frame_width, frame_height, frame_width, frame_height]
                ).astype(int)
                
                # Draw a white rectangle around the face
                cv2.rectangle(annotated_frame, face_rect, color=(255, 255, 255), thickness=2)

                # --- 2. Draw the Confidence Score ---
                confidence_score = detection.score[0]
                score_text = f"Confidence: {confidence_score:.2%}"
                cv2.putText(annotated_frame, score_text, (face_rect[0], face_rect[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # --- 3. Draw the 6 Key Facial Keypoints ---
                keypoints = detection.location_data.relative_keypoints
                for keypoint in keypoints:
                    keypoint_px = (int(keypoint.x * frame_width), int(keypoint.y * frame_height))
                    # Draw a small circle for each keypoint
                    cv2.circle(annotated_frame, keypoint_px, 4, (0, 255, 0), -1)
        else:
            # If no face is found, display a clear message
            cv2.putText(annotated_frame, "Face Not Detected", (50, 50), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)

        data = {
            'face_detected': face_detected,
            'success': True
        }
        
        return data, annotated_frame
Step 3: Update the Server Logic

Replace the entire content of the file EyeCareVision/server/enhanced_eye_tracking_server.py with the following code. This version is refactored to be more efficient and uses the new tracker.

Python

from flask import Flask, jsonify, request, Response
from flask_cors import CORS
from enhanced_eye_tracker import EnhancedEyeTracker
import cv2
import numpy as np
import threading
import time
from typing import Optional, Dict, Any

class EnhancedEyeTrackingServer:
    def __init__(self):
        self.app = Flask(__name__)
        # Configure CORS to allow requests from any origin to your API endpoints
        CORS(self.app, resources={r"/api/*": {"origins": "*"}})
        
        # Initialize with the new high-accuracy tracker
        self.eye_tracker = EnhancedEyeTracker(min_detection_confidence=0.7)
        
        self.cap: Optional[cv2.VideoCapture] = None
        self.camera_active = False
        self.tracking_active = False
        self.current_frame: Optional[np.ndarray] = None
        self.frame_lock = threading.Lock()
        
        self.last_detection_result: Optional[Dict[str, Any]] = None
        
        self.frame_thread: Optional[threading.Thread] = None
        self.frame_thread_active = False

        self.setup_routes()
    
    def _read_frames(self):
        """
        A dedicated background thread that continuously reads frames from the camera,
        processes them using the eye tracker, and updates the shared state.
        This is highly efficient as processing happens only once per frame.
        """
        print("Starting frame reading and processing thread.")
        while self.frame_thread_active and self.camera_active:
            if not (self.cap and self.cap.isOpened()):
                time.sleep(0.1)
                continue
            
            ret, frame = self.cap.read()
            if not ret or frame is None:
                print("Failed to capture a frame from the camera.")
                time.sleep(0.1)
                continue
            
            # Flip for a natural, mirror-like view
            frame = cv2.flip(frame, 1)
            
            annotated_frame = frame
            
            if self.tracking_active:
                # This single, efficient call gets both data and the visualized frame
                result, annotated_frame = self.eye_tracker.process_and_draw_frame(frame)
                self.last_detection_result = result
            else:
                # Clear old data if tracking is turned off
                self.last_detection_result = None
            
            # Safely update the frame that will be streamed to the frontend
            with self.frame_lock:
                self.current_frame = annotated_frame
            
            time.sleep(0.033) # Maintain a steady ~30 FPS
        print("Frame reading thread has stopped.")

    def setup_routes(self):
        """Defines all the API endpoints for the frontend to interact with."""

        @self.app.route('/api/enhanced-eye-tracking/status', methods=['GET'])
        def status():
            """Provides the current status of the server."""
            return jsonify({
                'status': 'running',
                'camera_status': 'active' if self.camera_active else 'inactive',
                'tracking_active': self.tracking_active
            })

        @self.app.route('/api/enhanced-eye-tracking/start_camera', methods=['POST'])
        def start_camera():
            """Starts the camera and the background frame-reading thread."""
            if self.camera_active:
                return jsonify({'success': True, 'message': 'Camera is already active'})
            
            self.cap = cv2.VideoCapture(0) # Use camera index 0
            if self.cap and self.cap.isOpened():
                self.camera_active = True
                self.frame_thread_active = True
                self.frame_thread = threading.Thread(target=self._read_frames)
                self.frame_thread.daemon = True
                self.frame_thread.start()
                return jsonify({'success': True, 'message': 'Camera started successfully'})
            else:
                return jsonify({'success': False, 'message': 'Failed to open camera'}), 500

        @self.app.route('/api/enhanced-eye-tracking/stop_camera', methods=['POST'])
        def stop_camera():
            """Stops the camera and cleans up resources."""
            if self.camera_active:
                self.frame_thread_active = False # Signal the thread to stop
                if self.frame_thread: self.frame_thread.join(timeout=1)
                if self.cap: self.cap.release()
                
                self.camera_active = False
                self.tracking_active = False
                self.cap = None
            return jsonify({'success': True, 'message': 'Camera stopped successfully'})

        @self.app.route('/api/enhanced-eye-tracking/start_tracking', methods=['POST'])
        def start_tracking():
            """Enables face detection processing on the video stream."""
            if not self.camera_active:
                return jsonify({'success': False, 'message': 'Camera is not active'}), 400
            self.tracking_active = True
            return jsonify({'success': True, 'message': 'Face detection tracking started'})

        @self.app.route('/api/enhanced-eye-tracking/stop_tracking', methods=['POST'])
        def stop_tracking():
            """Disables face detection processing."""
            self.tracking_active = False
            return jsonify({'success': True, 'message': 'Face detection tracking stopped'})

        @self.app.route('/api/enhanced-eye-tracking/get_enhanced_gaze', methods=['GET'])
        def get_enhanced_gaze():
            """Returns the latest face detection result."""
            if not self.tracking_active or self.last_detection_result is None:
                # Return a default "not detected" status
                return jsonify({'success': True, 'face_detected': False})
            return jsonify(self.last_detection_result)

        @self.app.route('/api/enhanced-eye-tracking/video_feed')
        def video_feed():
            """Streams the visually annotated video feed to the frontend."""
            def generate_frames():
                while self.camera_active:
                    frame_to_send = None
                    with self.frame_lock:
                        if self.current_frame is not None:
                            frame_to_send = self.current_frame
                    
                    if frame_to_send is not None:
                        ret, buffer = cv2.imencode('.jpg', frame_to_send)
                        if ret:
                            frame_bytes = buffer.tobytes()
                            yield (b'--frame\r\n'
                                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                    time.sleep(0.033) # Stream at ~30 FPS
            
            return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')
    
    def run(self, host='0.0.0.0', port=5002, debug=False):
        """Starts the Flask web server."""
        print(f"Starting Enhanced Eye Tracking Server on http://{host}:{port}")
        self.app.run(host=host, port=port, debug=debug, threaded=True, use_reloader=False)

if __name__ == '__main__':
    server = EnhancedEyeTrackingServer()
    server.run()
Step 4: Verification

After applying the changes, run the application using the run.sh script or by manually starting the servers. Open the web application and enable the camera and tracking. You should now see the video feed with the following behavior:

When your face is in view, a white bounding box, green keypoints, and a confidence score will be drawn on it.

When your face is not in view, a red "Face Not Detected" message will appear on the video feed.