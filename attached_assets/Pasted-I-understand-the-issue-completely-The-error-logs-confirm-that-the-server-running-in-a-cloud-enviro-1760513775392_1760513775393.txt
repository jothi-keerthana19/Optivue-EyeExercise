I understand the issue completely. The error logs confirm that the server, running in a cloud environment, cannot directly access your local webcam. This is a common and expected limitation.

The correct and standard solution is to let the browser handle the camera. Your frontend application will capture the video, and then send frames to the Python backend for processing. The backend will analyze the image and send back the results.

Here is the complete, rectified code that implements this proper architecture. This will solve the camera access problem and give you the accurate face detection with bounding boxes that you want.

## 1. server/enhanced_eye_tracker.py (Final, Corrected Code)
This file remains focused on high-accuracy image processing. It takes an image frame as input and returns the data and the annotated image. It no longer needs to worry about where the camera is.

Python

import cv2
import numpy as np
import mediapipe as mp
from typing import Dict, Tuple, Any

class EnhancedEyeTracker:
    """
    Processes an individual image frame to detect faces with high accuracy using MediaPipe.
    This class is decoupled from the video source and focuses purely on computer vision.
    """

    def __init__(self, model_selection: int = 1, min_detection_confidence: float = 0.7) -> None:
        """Initializes the high-accuracy FaceDetection model."""
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=model_selection,
            min_detection_confidence=min_detection_confidence
        )
        print(f"MediaPipe FaceDetection initialized with confidence={min_detection_confidence}")

    def process_and_draw_frame(self, frame: np.ndarray) -> Tuple[Dict[str, Any], np.ndarray]:
        """
        Analyzes a single frame for faces and draws visual feedback.

        Args:
            frame (np.ndarray): The input image frame (in BGR format).

        Returns:
            A tuple containing:
            - A dictionary with detection results ('face_detected', 'success').
            - The annotated frame with a bounding box and confidence score.
        """
        annotated_frame = frame.copy()
        frame_height, frame_width, _ = annotated_frame.shape
        rgb_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        
        results = self.face_detection.process(rgb_frame)
        
        face_detected = False
        if results.detections:
            face_detected = True
            for detection in results.detections:
                bbox_data = detection.location_data.relative_bounding_box
                face_rect = np.multiply(
                    [bbox_data.xmin, bbox_data.ymin, bbox_data.width, bbox_data.height],
                    [frame_width, frame_height, frame_width, frame_height]
                ).astype(int)
                
                cv2.rectangle(annotated_frame, face_rect, color=(255, 255, 255), thickness=2)
                score_text = f"Confidence: {detection.score[0]:.2%}"
                cv2.putText(annotated_frame, score_text, (face_rect[0], face_rect[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        else:
            cv2.putText(annotated_frame, "Face Not Detected", (50, 50), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)

        return {'face_detected': face_detected, 'success': True}, annotated_frame

## 2. server/enhanced_eye_tracking_server.py (Final, Corrected Code)
This is the fully revised server. It no longer tries to access a camera. Instead, it has a new endpoint, /process_frame, which receives images from your frontend.

Python

from flask import Flask, jsonify, request, Response
from flask_cors import CORS
from enhanced_eye_tracker import EnhancedEyeTracker
import cv2
import numpy as np
import threading
import time
from typing import Optional, Dict, Any

class EnhancedEyeTrackingServer:
    def __init__(self):
        self.app = Flask(__name__)
        CORS(self.app, resources={r"/api/*": {"origins": "*"}})
        
        self.eye_tracker = EnhancedEyeTracker(min_detection_confidence=0.7)
        
        # This server no longer needs direct camera access
        self.last_processed_frame: Optional[np.ndarray] = None
        self.frame_lock = threading.Lock()

        self.setup_routes()
    
    def setup_routes(self):
        """Defines API endpoints. The server now processes frames sent from the client."""

        @self.app.route('/api/enhanced-eye-tracking/status', methods=['GET'])
        def status():
            return jsonify({'status': 'running', 'message': 'Ready to process frames.'})

        @self.app.route('/api/enhanced-eye-tracking/process_frame', methods=['POST'])
        def process_frame():
            """
            Receives a frame from the frontend, processes it for face detection,
            and returns the data. This is the new core of the backend logic.
            """
            if 'frame' not in request.files:
                return jsonify({'error': "No 'frame' file part in the request."}), 400

            file = request.files['frame']
            
            # Convert the image file to an OpenCV frame
            try:
                np_img = np.frombuffer(file.read(), np.uint8)
                frame = cv2.imdecode(np_img, cv2.IMREAD_COLOR)
                if frame is None:
                    return jsonify({'error': 'Failed to decode image.'}), 400
            except Exception as e:
                return jsonify({'error': f'Error processing image: {str(e)}'}), 500

            # Perform the actual face detection
            result, annotated_frame = self.eye_tracker.process_and_draw_frame(frame)

            # Store the latest annotated frame for the video feed
            with self.frame_lock:
                self.last_processed_frame = annotated_frame

            return jsonify(result)

        @self.app.route('/api/enhanced-eye-tracking/video_feed')
        def video_feed():
            """
            Streams the last frame that was processed, now with bounding boxes drawn on it.
            This provides a live visual confirmation of what the AI is seeing.
            """
            def generate_frames():
                while True:
                    frame_to_send = None
                    with self.frame_lock:
                        if self.last_processed_frame is not None:
                            frame_to_send = self.last_processed_frame
                    
                    if frame_to_send is not None:
                        ret, buffer = cv2.imencode('.jpg', frame_to_send)
                        if ret:
                            frame_bytes = buffer.tobytes()
                            yield (b'--frame\r\n'
                                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                    # Stream at a consistent rate
                    time.sleep(0.05)
            
            return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')
    
    def run(self, host='0.0.0.0', port=5002, debug=False):
        print(f"Starting Enhanced Eye Tracking Server on http://{host}:{port}")
        self.app.run(host=host, port=port, debug=debug, threaded=True, use_reloader=False)

if __name__ == '__main__':
    server = EnhancedEyeTrackingServer()
    server.run()
## 3. Frontend (client/src/lib/eyeTracking.ts) (Final, Corrected Code)
Finally, we update the frontend API library to use the new architecture. It now sends frames to the server instead of just polling for data.

TypeScript

// client/src/lib/eyeTracking.ts

const API_BASE_URL = 'http://127.0.0.1:5002/api/enhanced-eye-tracking';

/**
 * Sends a single video frame (as a Blob) to the backend for processing.
 * This is the new core function for real-time interaction.
 *
 * @param frameBlob The video frame captured from the canvas as a Blob.
 * @returns A promise that resolves with the face detection data from the server.
 */
export const processFrame = async (frameBlob: Blob) => {
  try {
    const formData = new FormData();
    formData.append('frame', frameBlob, 'frame.jpg');

    const response = await fetch(`${API_BASE_URL}/process_frame`, {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(errorData.message || 'Failed to process frame');
    }
    return await response.json();
  } catch (error) {
    console.warn('Error processing frame:', error);
    // Return a default "not detected" state to prevent UI crashes
    return { face_detected: false, success: false };
  }
};

/**
 * Checks the status of the backend server.
 */
export const getServerStatus = async () => {
  try {
    const response = await fetch(`${API_BASE_URL}/status`);
    return await response.json();
  } catch (error) {
    console.error('Error getting server status:', error);
    return { status: 'error', message: 'Server is unreachable' };
  }
};
This corrected architecture is robust, efficient, and is the standard way to build computer vision web applications. It will solve your camera errors and provide the accurate, real-time feedback you need.













Tools

2.5 Pro

